{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36264bit36fc170516d04460ae12d1efa3e0666b",
   "display_name": "Python 3.6.2 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Files already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\n"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.transforms.transforms as T\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "TRAIN_SET_NUM = 49000\n",
    "BATCH_SIZE = 64\n",
    "EPOCH_NUM = 15\n",
    "\n",
    "# 数据预处理\n",
    "transform_normal = T.Compose([\n",
    "    T.ToTensor(), \n",
    "    T.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# 数据增强\n",
    "transform_aug = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "\n",
    "# 加载训练集\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./', train=True, transform=transform_aug, download=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler.SubsetRandomSampler(range(TRAIN_SET_NUM)))\n",
    "\n",
    "# 加载验证集\n",
    "val_dataset = torchvision.datasets.CIFAR10(root='./', train=True, transform=transform_normal, download=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=sampler.SubsetRandomSampler(range(TRAIN_SET_NUM, 50000)))\n",
    "\n",
    "# 加载测试集\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./', train=False, transform=transform_normal, download=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "using device: cuda\n"
    }
   ],
   "source": [
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ResNet18(\n  (conv1): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (layer1): Sequential(\n    (0): ResidualBlock(\n      (left): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (shortcut): Sequential()\n    )\n    (1): ResidualBlock(\n      (left): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (shortcut): Sequential()\n    )\n  )\n  (layer2): Sequential(\n    (0): ResidualBlock(\n      (left): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (shortcut): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): ResidualBlock(\n      (left): Sequential(\n        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (shortcut): Sequential()\n    )\n  )\n  (layer3): Sequential(\n    (0): ResidualBlock(\n      (left): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (shortcut): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): ResidualBlock(\n      (left): Sequential(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (shortcut): Sequential()\n    )\n  )\n  (layer4): Sequential(\n    (0): ResidualBlock(\n      (left): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (shortcut): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): ResidualBlock(\n      (left): Sequential(\n        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (shortcut): Sequential()\n    )\n  )\n  (fc): Linear(in_features=512, out_features=10, bias=True)\n)\n"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, ic, oc, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # 残差\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(ic, oc, kernel_size = 3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(oc),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(oc, oc, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(oc)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # residual block 经过一次降采样 通道数翻倍\n",
    "        # 若通道未翻倍，则为普通卷积，不降采样，也不翻倍通道数\n",
    "        # stride == 1 || ic == oc未降采样\n",
    "        if stride != 1 or ic != oc:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(ic, oc, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(oc)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, ResidualBlock, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64,  2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        \"\"\"\n",
    "        每个 layer 由多个 residual block 组成\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            if i == 0:\n",
    "                layers.append(block(self.inchannel, channels, stride))\n",
    "            else:\n",
    "                layers.append(block(channels, channels, 1))\n",
    "            self.inchannel = channels\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net = ResNet18(ResidualBlock)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "train_loss_hist = []\n",
    "test_loss_hist = []\n",
    "\n",
    "# 验证模型在验证集或者测试集上的准确率\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()   # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            scores = model(x)\n",
    "            _,preds = scores.max(1)\n",
    "            num_correct += (preds==y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 *acc ))\n",
    "        return acc\n",
    "\n",
    "def train_model(model, optimizer, epochs=1, scheduler=None):\n",
    "    '''\n",
    "    Parameters:\n",
    "    - model: A Pytorch Module giving the model to train.\n",
    "    - optimizer: An optimizer object we will use to train the model\n",
    "    - epochs: A Python integer giving the number of epochs to train\n",
    "    Returns: best model\n",
    "    '''\n",
    "    best_model_wts = None\n",
    "    best_acc = 0.0\n",
    "    model = model.to(device=device) # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        for t,(x,y) in enumerate(train_dataloader):\n",
    "            model.train()   # set model to training mode\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch %d, loss=%.4f' % (e, loss.item()))\n",
    "        acc = check_accuracy(val_dataloader, model)\n",
    "        if acc > best_acc:\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            best_acc = acc\n",
    "    print('best_acc:',best_acc)\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0, loss=0.8930\nChecking accuracy on validation set\nGot 547 / 1000 correct (54.70)\nEpoch 1, loss=0.7152\nChecking accuracy on validation set\nGot 726 / 1000 correct (72.60)\nEpoch 2, loss=0.4884\nChecking accuracy on validation set\nGot 781 / 1000 correct (78.10)\nEpoch 3, loss=0.4800\nChecking accuracy on validation set\nGot 792 / 1000 correct (79.20)\nEpoch 4, loss=0.6150\nChecking accuracy on validation set\nGot 834 / 1000 correct (83.40)\nEpoch 5, loss=0.4651\nChecking accuracy on validation set\nGot 837 / 1000 correct (83.70)\nEpoch 6, loss=0.2091\nChecking accuracy on validation set\nGot 852 / 1000 correct (85.20)\nEpoch 7, loss=0.4268\nChecking accuracy on validation set\nGot 861 / 1000 correct (86.10)\nEpoch 8, loss=0.1468\nChecking accuracy on validation set\nGot 866 / 1000 correct (86.60)\nEpoch 9, loss=0.1507\nChecking accuracy on validation set\nGot 882 / 1000 correct (88.20)\nEpoch 10, loss=0.4157\nChecking accuracy on validation set\nGot 868 / 1000 correct (86.80)\nEpoch 11, loss=0.1216\nChecking accuracy on validation set\nGot 886 / 1000 correct (88.60)\nEpoch 12, loss=0.2511\nChecking accuracy on validation set\nGot 882 / 1000 correct (88.20)\nEpoch 13, loss=0.3760\nChecking accuracy on validation set\nGot 888 / 1000 correct (88.80)\nEpoch 14, loss=0.2444\nChecking accuracy on validation set\nGot 881 / 1000 correct (88.10)\nEpoch 15, loss=0.0532\nChecking accuracy on validation set\nGot 923 / 1000 correct (92.30)\nEpoch 16, loss=0.0217\nChecking accuracy on validation set\nGot 926 / 1000 correct (92.60)\nEpoch 17, loss=0.2670\nChecking accuracy on validation set\nGot 923 / 1000 correct (92.30)\nEpoch 18, loss=0.0275\nChecking accuracy on validation set\nGot 925 / 1000 correct (92.50)\nEpoch 19, loss=0.1350\nChecking accuracy on validation set\nGot 927 / 1000 correct (92.70)\nbest_acc: 0.927\n"
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=15,gamma=0.1)\n",
    "best_model = train_model(net, optimizer, epochs=EPOCH_NUM, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Checking accuracy on test set\nGot 9103 / 10000 correct (91.03)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9103"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "check_accuracy(test_dataloader, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 91.03%"
   ]
  }
 ]
}