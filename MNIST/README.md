# 梯度爆炸/梯度消失

学习率过大

# loss 不收敛

没有对数据做归一化。没有检查过你的结果。这里的结果包括预处理结果和最终的训练测试结果。忘了做数据预处理。忘了使用正则化。Batch Size设的太大。学习率设的不对。最后一层的激活函数用的不对。网络存在坏梯度。比如Relu对负值的梯度为0，反向传播时，0梯度就是不传播。参数初始化错误。网络太深。隐藏层神经元数量错误。

# 加速收敛 

批标准化

https://www.cnblogs.com/guoyaohua/p/8724433.html


relu的主要作用还是减轻梯度爆炸的可能性

隐藏层个数